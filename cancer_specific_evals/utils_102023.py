
# Attention layer

# from tensorflow.keras.preprocessing import sequence
# from tensorflow.keras.models import Sequential, Model
# from tensorflow.keras.layers import Input, Dense, Dropout, Activation, concatenate
# from tensorflow.keras.layers import Embedding
# from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers

# from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, TimeDistributed, GRU, Bidirectional, Layer


#import itertools
import numpy as np
import matplotlib.pyplot as plt


from IPython.core.debugger import set_trace



def sigmoid(x):
    return 1. / (1. + np.exp(-x))





def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """

    import itertools
    import numpy as np

    from sklearn.metrics import confusion_matrix
  
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.grid(False)
    plt.show()

    
    
# now eval model function

def eval_model(predicted, actual, graph=False):
    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import f1_score
    from sklearn.metrics import classification_report
    from sklearn.metrics import precision_recall_curve
    from sklearn.metrics import auc
    from sklearn.metrics import roc_curve
    from sklearn.metrics import confusion_matrix
    from sklearn.calibration import calibration_curve
    import matplotlib.pyplot as plt
    import matplotlib.lines as mlines
    import matplotlib.transforms as mtransforms
    
    outcome_counts = np.unique(actual, return_counts=True)[1]
    try:
        prob_outcome = outcome_counts[1] / (outcome_counts[0] + outcome_counts[1])
        print("AUC " + str(roc_auc_score(actual, predicted)))


        # calculate the fpr and tpr for all thresholds of the classification
        fpr, tpr, threshold = roc_curve(actual, predicted)
        roc_auc = auc(fpr, tpr)

        from sklearn.metrics import average_precision_score
        average_precision = average_precision_score(actual, predicted)

        print('Outcome probability: ' + str(prob_outcome))


        print('Average precision score: {0:0.2f}'.format(
            average_precision))

        # best F1
        precision, recall, thresholds = precision_recall_curve(actual, predicted)


        F1 = 2*((precision*recall)/(precision+recall))
        print("Best F1: " + str(max(F1)))


        # threshold for best F1
        bestF1_thresh = thresholds[np.argmax(F1)]

        if graph==False:
            print('Best F1 threshold: ' + str(bestF1_thresh))
            return bestF1_thresh

        # method I: plt
        import matplotlib.pyplot as plt
        plt.title('Receiver Operating Characteristic: ' )
        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        plt.legend(loc = 'lower right')
        plt.plot([0, 1], [0, 1],'r--')
        plt.xlim([0, 1])
        plt.ylim([0, 1])
        plt.ylabel('True Positive Rate')
        plt.xlabel('False Positive Rate')
        plt.show()



        import matplotlib.pyplot as plt
        ##from sklearn.utils.fixes import signature


        # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument
    #     step_kwargs = ({'step': 'post'}
    #                  if 'step' in signature(plt.fill_between).parameters
    #                  else {})




        plt.plot(recall, precision, color='b')
        plt.plot([0,1],[prob_outcome,prob_outcome], 'r--')
        plt.step(recall, precision, color='b', alpha=0.2,
                 where='post')
        plt.fill_between(recall, precision, alpha=0.2, color='b')

        plt.xlabel('Recall (Sensitivity)')
        plt.ylabel('Precision (PPV)')
        plt.ylim([0.0, 1.05])
        plt.xlim([0.0, 1.0])
        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(
                average_precision))
        plt.show()



        print("Threshold for best F1:")
        print(bestF1_thresh)
        pred_outcome_best_f1_thresh = np.where(predicted >= bestF1_thresh,1,0)
        pred_outcome_05_thresh = np.where(predicted >= 0.5,1,0)

        # # predictions

        # # confusion matrix
        print("Confusion matrix at best F1 thresh:")

        cnf_matrix = confusion_matrix(actual, pred_outcome_best_f1_thresh)
        np.set_printoptions(precision=2)
        # Plot non-normalized confusion matrix
        plt.figure()
        plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],
                            title='Confusion matrix, without normalization')
        print("Metrics at best F1 thresh (specificity is recall for negative class):")
        from sklearn.metrics import classification_report
        print(classification_report(actual, pred_outcome_best_f1_thresh, target_names=['No','Yes']))


        print("Confusion matrix at 0.5 thresh:")
        from sklearn.metrics import confusion_matrix
        cnf_matrix = confusion_matrix(actual, pred_outcome_05_thresh)
        np.set_printoptions(precision=2)
        # Plot non-normalized confusion matrix
        plt.figure()
        plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],
                            title='Confusion matrix, without normalization')
        print("Metrics at 0.5 thresh thresh (specificity is recall for negative class):")
        print(classification_report(actual, pred_outcome_05_thresh, target_names=['No','Yes']))



        # Plot normalized confusion matrix
        #   plt.figure()
        #   plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
        #                         title='Normalized confusion matrix')


        # # plot threshold vs ppv curve
        plt.plot(thresholds, precision[0:len(precision)-1], color='b')
        # plt.step(recall, precision, color='b', alpha=0.2,
        #          where='post')
        # plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)

        plt.xlabel('Threshold probability')
        plt.ylabel('Precision (PPV)')
        plt.ylim([0.0, 1.0])
        plt.xlim([0.0, 1.0])
        plt.title('Threshold vs precision')
        plt.show()

        # histogram
        plt.hist(predicted)
        plt.title("Histogram")
        plt.xlabel("Predicted probability" )
        plt.ylabel("Frequency")
        plt.show()
        
        
        # calibration curve
        y_plot, x_plot = calibration_curve(actual, sigmoid(predicted), n_bins=15)
        fig, ax = plt.subplots()
        # only these two lines are calibration curves
        plt.plot(x_plot, y_plot, marker='o', linewidth=1, label='calibration')

        # reference line, legends, and axis labels
        line = mlines.Line2D([0, 1], [0, 1], color='black')
        transform = ax.transAxes
        line.set_transform(transform)
        ax.add_line(line)
        fig.suptitle('Calibration plot')
        ax.set_xlabel('Predicted probability')
        ax.set_ylabel('True probability in each bin')
        plt.legend()
        plt.show()

        return bestF1_thresh
    
    except:
        print("no outcome variation to calculate metrics")
        return None
    
